{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupied-aging",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "-Preprocessing- \\\n",
    "Missing value \\\n",
    "Normalization\n",
    "\n",
    "\n",
    "-Ensemble model- \\\n",
    "Random Forest \\\n",
    "XGBoost \\\n",
    "CatBoost \\\n",
    "LightGBM \\\n",
    "\n",
    "-Hyperparameters- \\\n",
    "Optuna\n",
    "\n",
    "-Evaluation- \\\n",
    "C-index? \\\n",
    "Shap\n",
    "\n",
    "ãƒ»Problem \\\n",
    "How to approach long run prognosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "junior-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pregnosis_tool import c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "obvious-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pregnosis:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def preprocessing(self, df, return_df = False):\n",
    "        categorical = df.select_dtypes(\"object\")\n",
    "        numerical = df.select_dtypes(exclude=[\"object\"])\n",
    "        print(f\"Rows Number: {df.shape[0]}\")\n",
    "        print(f\"Columns Number: {df.shape[1]} \\n\")\n",
    "        print(f\"Variables List: \\n{list(df.columns)}\\n\")\n",
    "        print(f\"--Categorical--: \\n{list(categorical.columns)} \\n\")\n",
    "        print(f\"--Numerical--: \\n{list(numerical.columns)} \\n\")\n",
    "        print(f\"Number of NaN: \\n{df.isnull().sum()}\\n\")\n",
    "        print(f\"Data types: \\n{df.dtypes}\")\n",
    "        if return_df == True:\n",
    "            return categorical, numerical\n",
    "        \n",
    "    def missing(self, df):\n",
    "        import seaborn as sns\n",
    "        #frac_missing = df.isnull().sum()/len(df)\n",
    "        percent_missing = df.isnull().sum()*100/len(df)\n",
    "        return percent_missing.sort_values(ascending=False), sns.heatmap(df.isnull(), cbar=False)\n",
    "    \n",
    "    \n",
    "    def mean_imputer(self, df):\n",
    "        '''This func takes dataset with missing values, \n",
    "            and impute them with mean values'''\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        mean_imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "        imputed_df = mean_imputer.fit_transform(df)\n",
    "        return imputed_df    \n",
    "\n",
    "    def regression_imputer(self, df):\n",
    "        '''This func takes dataset with missing values, \n",
    "            and impute them with regression values'''\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "        reg_imputer = IterativeImputer()\n",
    "        imputed_df = reg_imputer.fit_transform(df)\n",
    "        return imputed_df\n",
    "    \n",
    "    def logistic_classification(self, df, targe_variable):\n",
    "        '''This func split data into train and test, then train into train and valid\n",
    "            to cross-validate with grid-search and return the algorithm?'''\n",
    "        from sklean.linear_model import LogisticRegression\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        \n",
    "        y = df.pop(target_variable).values\n",
    "        X = df.values\n",
    "        \n",
    "        classifier = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "        params_grid = {'logistic_C': np.logspace(-4,4,4)}\n",
    "        search = GridSearchCV(classifier, params_grid, njobs=-1)\n",
    "        search.fit(X, y)\n",
    "        print(\"Best parameter (CV score=%0.3f):\"%search.best_score_)\n",
    "        print(search.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "empirical-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Animal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "palestinian-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'str'>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "hydraulic-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f notebook_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collected-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "republican-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\"People\":[\"John\", \"Mirrar\",\"Sarr\",\"He\", \"Mosso\", \"Kota\", \"Kin\", \"Wellily\"], \"Age\": [22,45,12,33,73,12,55,41]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "essential-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:/users/daisu/OneDrive/Desktop/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advised-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Number: 891\n",
      "Columns Number: 12 \n",
      "\n",
      "Variables List: \n",
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "\n",
      "--Categorical--: \n",
      "['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'] \n",
      "\n",
      "--Numerical--: \n",
      "['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'] \n",
      "\n",
      "Number of NaN: \n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "Data types: \n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from prognosis_tool import preprocessing, add_interactions, regression_imputer\n",
    "\n",
    "categorical, numerical = preprocessing(df2, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "electric-glasgow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_interactions(X):\n",
      "    \"\"\"\n",
      "    Add interaction terms between columns to dataframe.\n",
      "\n",
      "    Args:\n",
      "    X (dataframe): Original data\n",
      "\n",
      "    Returns:\n",
      "    X_int (dataframe): Original data with interaction terms appended. \n",
      "    \"\"\"\n",
      "    features = X.columns\n",
      "    m = len(features)\n",
      "    X_int = X.copy(deep=True)\n",
      "\n",
      "    # 'i' loops through all features in the original dataframe X\n",
      "    for i in range(m):\n",
      "        \n",
      "        # get the name of feature 'i'\n",
      "        feature_i_name = features[i]\n",
      "        \n",
      "        # get the data for feature 'i'\n",
      "        feature_i_data = X_int[feature_i_name]\n",
      "        # choose the index of column 'j' to be greater than column i\n",
      "        for j in range(i+1, m):\n",
      "            \n",
      "            # get the name of feature 'j'\n",
      "            feature_j_name = features[j]\n",
      "            \n",
      "            # get the data for feature j'\n",
      "            feature_j_data = X_int[feature_j_name]\n",
      "            \n",
      "            # create the name of the interaction feature by combining both names\n",
      "            # example: \"apple\" and \"orange\" are combined to be \"apple_x_orange\"\n",
      "            feature_i_j_name = f\"{features[i]}_x_{features[j]}\"\n",
      "            \n",
      "            # Multiply the data for feature 'i' and feature 'j'\n",
      "            # store the result as a column in dataframe X_int\n",
      "            X_int[feature_i_j_name] = feature_i_data*feature_j_data\n",
      "    return X_int\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(add_interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consistent-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical2 = regression_imputer(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indonesian-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = add_interactions(numerical2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "loose-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    categorical = list(df.select_dtypes(\"object\").columns)\n",
    "    numerical = list(df.select_dtypes(exclude=[\"object\"]).columns)\n",
    "    print(f\"Rows Number: {df.shape[0]}\")\n",
    "    print(f\"Columns Number: {df.shape[1]} \\n\")\n",
    "    print(f\"Variables List: \\n{list(df.columns)}\\n\")\n",
    "    print(f\"Categorical: \\n{categorical} \\n\")\n",
    "    print(f\"Numerical: \\n{numerical} \\n\")\n",
    "    print(f\"Number of NaN: \\n{df.isnull().sum()}\\n\")\n",
    "    print(f\"Data types: \\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "medical-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "macro-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.67 Âµs Â± 291 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit py_fib2(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
